#!/usr/bin/env bash
set -euo pipefail

usage() {
  echo "Usage: $0 <input-file-with-presigned-urls-or-relative-paths> <output-dir> [baseurl]"
  echo "  If baseurl is given, input file lines are treated as relative paths and baseurl is prepended to form full URLs."
  echo "  If baseurl is not given, input file lines are treated as full URLs."
  exit 1
}

if [ $# -lt 2 ] || [ $# -gt 3 ]; then
  usage
fi

INPUT="$1"
OUTPUT_DIR="$2"
BASEURL="${3:-}"

if [[ "$INPUT" != http* ]] && [ ! -f "$INPUT" ]; then
  echo "[ERROR] Input file '$INPUT' does not exist." >&2
  exit 2
fi

mkdir -p "$OUTPUT_DIR"

# If INPUT is a single URL, write to a temp file for processing
TMP_URL_FILE=""
if [[ "$INPUT" =~ ^https?:// ]]; then
  TMP_URL_FILE=$(mktemp)
  echo "$INPUT" > "$TMP_URL_FILE"
  INPUT="$TMP_URL_FILE"
fi

URL_COUNT=$(grep -vE '^\s*#' "$INPUT" | grep -cve '^\s*$' || echo 0)
if (( URL_COUNT > 100000 )); then
  echo "[WARNING] Input file contains $URL_COUNT URLs or paths."
  echo "Parsing and directory creation may take a long time. Estimated time depends on storage speed and CPU."
fi

TMP_ARIA2_INPUT=$(mktemp)

echo "[INFO] Parsing inputs and preparing download inputs..."

if [ -z "$BASEURL" ]; then
  # Mode 1: Input lines are full URLs
  # Extract path after domain from each URL, create directories accordingly
  awk -v output_dir="$OUTPUT_DIR" '
  {
    gsub(/^[ \t]+|[ \t]+$/, "", $0)
    if ($0 ~ /^#/ || length($0) == 0) next

    url = $0
    split(url, a, "?")
    url_no_query = a[1]

    # Extract path after domain
    sub(/^https?:\/\/[^\/]+\//, "", url_no_query)
    out_path = output_dir "/" url_no_query

    dir = out_path
    sub(/[^\/]+$/, "", dir)
    if (dir == "") dir = output_dir

    file = out_path
    sub(/^.*\//, "", file)

    print url "\t" dir "\t" file
  }' "$INPUT" > "$TMP_ARIA2_INPUT.parsed"

else
  # Mode 2: Input lines are relative paths, prepend baseurl to form full URLs
  awk -v output_dir="$OUTPUT_DIR" -v baseurl="$BASEURL" '
  {
    gsub(/^[ \t]+|[ \t]+$/, "", $0)
    if ($0 ~ /^#/ || length($0) == 0) next

    rel_path = $0

    # Compose full URL: ensure baseurl ends with /
    if (substr(baseurl, length(baseurl)) != "/") {
      baseurl = baseurl "/"
    }
    url = baseurl rel_path

    out_path = output_dir "/" rel_path

    dir = out_path
    sub(/[^\/]+$/, "", dir)
    if (dir == "") dir = output_dir

    file = out_path
    sub(/^.*\//, "", file)

    print url "\t" dir "\t" file
  }' "$INPUT" > "$TMP_ARIA2_INPUT.parsed"
fi

# Create directories (unique) in parallel
echo "[INFO] Creating directories..."
cut -f2 "$TMP_ARIA2_INPUT.parsed" | sort -u | xargs -P 8 -I{} mkdir -p "{}"

echo "[INFO] Preparing aria2 input file..."
{
  while IFS=$'\t' read -r url dir file; do
    echo "$url"
    echo "  out=$file"
    echo "  dir=$dir"
  done < "$TMP_ARIA2_INPUT.parsed"
} > "$TMP_ARIA2_INPUT"

rm -f "$TMP_ARIA2_INPUT.parsed"

if [ -n "$TMP_URL_FILE" ]; then
  rm -f "$TMP_URL_FILE"
fi

echo "[INFO] Starting downloads with aria2c..."

aria2c --max-concurrent-downloads=6 --split=1 --auto-file-renaming=false --allow-overwrite=true --retry-wait=3 --max-tries=3 -i "$TMP_ARIA2_INPUT"

rm -f "$TMP_ARIA2_INPUT"

echo "[INFO] Download complete."
